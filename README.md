# awesome-tuning

This repo is a collection of AWESOME things about tuning methods and pretraining-free methods, including papers, code, etc.


![](https://img.shields.io/github/last-commit/demoleiwang/awesome-mixup?color=green) ![](https://img.shields.io/badge/PaperNumber-30-brightgreen) ![](https://img.shields.io/badge/PRs-Welcome-red) 

<!-- 
1. **[[]]()** x. x. [[code](x)] 

    *x* 

-->

### ICLR 2020

1. **[[Mixout'20]](https://openreview.net/pdf?id=HkgaETNtDB)**  Mixout: Effective Regularization to Finetune Large-scale Pretrained Language. ICLR 2020. [[code](https://github.com/bloodwass/mixout)] 

    *Cheolhyoung Lee, Kyunghyun Cho, Wanmo Kang.* 

1. **[[ELR'20]](http://www.openreview.net/pdf?id=B1g8VkHFPH)** Rethinking the Hyperparameters for Fine-tuning. ICLR 2020. [[code](x)] 

    *Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto.* 


### NeurIPS 2020

1. **[[Co-Tuning'20]](https://proceedings.neurips.cc/paper/2020/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf)** Co-Tuning for Transfer Learning. NeurIPS 2020. [[code](https://github.com/thuml/CoTuning)] 

    *Kaichao You, Zhi Kou, Mingsheng Long, Jianmin Wang.* 

1. **[[Movement Pruning'20]](https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf)** Movement Pruning: Adaptive Sparsity by Fine-Tuning. NeurIPS 2020. [[code](x)] 

    *Victor Sanh, Thomas Wolf, Alexander Rush.* 


### ICLR 2021

1. **[[Bert-stable-fine-tuning'21]](https://openreview.net/forum?id=nzpLWnVAyah)** On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. ICLR 2021. [[code](github uds-lsv/bert-stable-fine-tuning)] [[Slides]](https://iclr.cc/media/iclr-2021/Slides/2558.pdf)

    *Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow.* 

1. **[[R3F'21]](https://openreview.net/forum?id=OQ08SN70M1V)** Better Fine-Tuning by Reducing Representational Collapse. ICLR 2021. 

    *Armen Aghajanyan · Akshat Shrivastava · Anchit Gupta · Naman Goyal · Luke Zettlemoyer · Sonal Gupta.* 

1. **[[SCL-FT'21]](https://openreview.net/forum?id=cu7IUiOhujH)** Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning. ICLR 2021. [[code](x)] 

    *Beliz Gunel · Jingfei Du · Alexis Conneau · Veselin Stoyanov.* 

1. **[[VIBERT'21]](https://openreview.net/forum?id=kvhzKz-_DMF)** Variational Information Bottleneck for Effective Low-Resource Fine-Tuning. ICLR 2021. [[code](github rabeehk/vibert)] 

    *Rabeeh Karimi Mahabadi · Yonatan Belinkov · James Henderson.* 

1. **[[RBF'21]](https://openreview.net/forum?id=cO1IH43yUF)** Revisiting Few-sample BERT Fine-tuning. ICLR 2021. [[code](https://github.com/asappresearch/revisit-bert-finetuning)] [[Slides]](https://iclr.cc/media/iclr-2021/Slides/2678.pdf)

    *Tianyi Zhang · Felix Wu · Arzoo Katiyar · Kilian Weinberger · Yoav Artzi.* 


### ICML 2021

1. **[[Zoo-Tuning'21]](http://proceedings.mlr.press/v139/shu21b.html)** Zoo-Tuning: Adaptive Transfer from A Zoo of Models. ICML 2021. [[Slides]](https://icml.cc/media/icml-2021/Slides/9887.pdf)

    *Yang Shu · Zhi Kou · Zhangjie Cao · Jianmin Wang · Mingsheng Long.* 

1. **[[CFT'21]](http://proceedings.mlr.press/v139/xie21f.html)** Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization. ICML 2021.  [[Slides](https://icml.cc/media/icml-2021/Slides/10589.pdf)]

    *Sang Michael Xie · Tengyu Ma · Percy Liang.* 



### NeurIPS 2021

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 

1. **[[]]()** x. NeurIPS 2021. [[code](x)] 

    *x* 



### ICLR 2022

1. **[[]]()** x. ICLR 2022. [[code](x)] 

    *x* 

1. **[[]]()** x. ICLR 2022. [[code](x)] 

    *x* 

1. **[[]]()** x. ICLR 2022. [[code](x)] 

    *x* 

1. **[[]]()** x. ICLR 2022. [[code](x)] 

    *x* 

1. **[[]]()** x. ICLR 2022. [[code](x)] 

    *x* 

1. **[[]]()** x. ICLR 2022. [[code](x)] 

    *x* 



### ICML 2022

1. **[[]]()** x. ICML 2022. [[code](x)] 

    *x* 

1. **[[]]()** x. ICML 2022. [[code](x)] 

    *x* 

1. **[[]]()** x. ICML 2022. [[code](x)] 

    *x* 

1. **[[]]()** x. ICML 2022. [[code](x)] 

    *x* 



### NeurIPS 2022 (TBD)


### ICLR 2023 (TBD)

