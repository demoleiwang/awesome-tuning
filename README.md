# awesome-tuning

This repo is a collection of AWESOME things about tuning methods and pretraining-free methods, including papers, code, etc.


![](https://img.shields.io/github/last-commit/demoleiwang/awesome-mixup?color=green) ![](https://img.shields.io/badge/PaperNumber-31-brightgreen) ![](https://img.shields.io/badge/PRs-Welcome-red) 

<!-- 
1. **[[]]()** x. x. [[code](x)] 

    *x* 

-->

### ICLR 2020

1. **[[Mixout'20]](https://openreview.net/pdf?id=HkgaETNtDB)**  Mixout: Effective Regularization to Finetune Large-scale Pretrained Language. ICLR 2020. [[code](https://github.com/bloodwass/mixout)] 

    *Cheolhyoung Lee, Kyunghyun Cho, Wanmo Kang.* 

1. **[[ELR'20]](http://www.openreview.net/pdf?id=B1g8VkHFPH)** Rethinking the Hyperparameters for Fine-tuning. ICLR 2020. [[code](x)] 

    *Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto.* 


### NeurIPS 2020

1. **[[Co-Tuning'20]](https://proceedings.neurips.cc/paper/2020/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf)** Co-Tuning for Transfer Learning. NeurIPS 2020. [[code](https://github.com/thuml/CoTuning)] 

    *Kaichao You, Zhi Kou, Mingsheng Long, Jianmin Wang.* 

1. **[[Movement Pruning'20]](https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf)** Movement Pruning: Adaptive Sparsity by Fine-Tuning. NeurIPS 2020. 

    *Victor Sanh, Thomas Wolf, Alexander Rush.* 


### ICLR 2021

1. **[[Bert-stable-fine-tuning'21]](https://openreview.net/forum?id=nzpLWnVAyah)** On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. ICLR 2021. [[code](https://github.com/uds-lsv/bert-stable-fine-tuning)] [[Slides]](https://iclr.cc/media/iclr-2021/Slides/2558.pdf)

    *Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow.* 

1. **[[R3F'21]](https://openreview.net/forum?id=OQ08SN70M1V)** Better Fine-Tuning by Reducing Representational Collapse. ICLR 2021. 

    *Armen Aghajanyan · Akshat Shrivastava · Anchit Gupta · Naman Goyal · Luke Zettlemoyer · Sonal Gupta.* 

1. **[[SCL-FT'21]](https://openreview.net/forum?id=cu7IUiOhujH)** Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning. ICLR 2021.

    *Beliz Gunel · Jingfei Du · Alexis Conneau · Veselin Stoyanov.* 

1. **[[VIBERT'21]](https://openreview.net/forum?id=kvhzKz-_DMF)** Variational Information Bottleneck for Effective Low-Resource Fine-Tuning. ICLR 2021. [[code](https://github.com/rabeehk/vibert)] 

    *Rabeeh Karimi Mahabadi · Yonatan Belinkov · James Henderson.* 

1. **[[RBF'21]](https://openreview.net/forum?id=cO1IH43yUF)** Revisiting Few-sample BERT Fine-tuning. ICLR 2021. [[code](https://github.com/asappresearch/revisit-bert-finetuning)] [[Slides]](https://iclr.cc/media/iclr-2021/Slides/2678.pdf)

    *Tianyi Zhang · Felix Wu · Arzoo Katiyar · Kilian Weinberger · Yoav Artzi.* 


### ICML 2021

1. **[[Zoo-Tuning'21]](http://proceedings.mlr.press/v139/shu21b.html)** Zoo-Tuning: Adaptive Transfer from A Zoo of Models. ICML 2021. [[Slides]](https://icml.cc/media/icml-2021/Slides/9887.pdf)

    *Yang Shu · Zhi Kou · Zhangjie Cao · Jianmin Wang · Mingsheng Long.* 

1. **[[CFT'21]](http://proceedings.mlr.press/v139/xie21f.html)** Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization. ICML 2021.  [[Slides](https://icml.cc/media/icml-2021/Slides/10589.pdf)]

    *Sang Michael Xie · Tengyu Ma · Percy Liang.* 



### NeurIPS 2021

1. **[[XX'21]](https://openreview.net/forum?id=MDMV2SxCboX)** Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning. NeurIPS 2021. 

    *Colin Wei, Sang Michael Xie, Tengyu Ma.* 

1. **[[Core-tuning'21]](https://openreview.net/forum?id=LY6qkvd71Td)** Unleashing the Power of Contrastive Self-Supervised Visual Models via Contrast-Regularized Fine-Tuning. NeurIPS 2021. [[code](https://github.com/Vanint/Core-tuning)] 

    *Yifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, Jiashi Feng.* 

1. **[[XX'21]](https://openreview.net/forum?id=4HDwT8l12UK)** How Fine-Tuning Allows for Effective Meta-Learning. NeurIPS 2021. 

    *Kurtland Chua, Qi Lei, Jason D. Lee.* 

1. **[[FEMNIST'21]](https://openreview.net/forum?id=p99rWde9fVJ)** Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing. NeurIPS 2021. [[code](https://github.com/mkhodak/fedex)] 

    *Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Nina Balcan, Virginia Smith, Ameet Talwalkar.* 

1. **[[XX'21]](https://openreview.net/forum?id=pl2WX3riyiq)** How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness? NeurIPS 2021.

    *Xinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng Yan, Hanwang Zhang.* 

1. **[[RSL'21]](https://openreview.net/forum?id=QX32YlxrQJc)** Improved Regularization and Robustness for Fine-tuning in Neural Networks. NeurIPS 2021. [[code](https://github.com/NEU-StatsML-Research/Regularized-Self-Labeling)] 

    *Dongyue Li, Hongyang Zhang.* 

1. **[[AdvCL]](https://openreview.net/forum?id=70kOIgjKhbA)** When does Contrastive Learning Preserve Adversarial Robustness from Pretraining to Finetuning? NeurIPS 2021. 

    *Lijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, Chuang Gan.* 

1. **[[XX'21]](https://openreview.net/forum?id=DyE8hmj2dse)** A Theoretical Analysis of Fine-tuning with Linear Teachers. NeurIPS 2021. 

    *Gal Shachaf, Alon Brutzkus, Amir Globerson.* 




### ICLR 2022

1. **[[flan'22]](https://openreview.net/pdf?id=gEZrGCozdqR)** Finetuned Language Models are Zero-Shot Learners. ICLR 2022. [[code](https://github.com/google-research/flan)] 

    *Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Andrew_M._Dai1, Quoc V Le.* 

1. **[[LP-FT'22]](https://openreview.net/pdf?id=UYneFzXSJWh)** Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. ICLR 2022. 

    *Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, Percy Liang.* 

1. **[[XX'22]](https://openreview.net/forum?id=Q42f0dfjECO)** Differentially Private Fine-tuning of Language Models. ICLR 2022. [[code](https://github.com/huseyinatahaninan/Differentially-Private-Fine-tuning-of-Language-Models)] 

    *Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang.* 

1. **[[]](https://openreview.net/forum?id=f2OYVDyfIB)** Scale Efficiently: Insights from Pretraining and Finetuning Transformers. ICLR 2022. 

    *Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler.* 

1. **[[RPT'22]](https://openreview.net/forum?id=eBCmOocUejf)** On Robust Prefix-Tuning for Text Classification. ICLR 2022. [[code](https://github.com/minicheshire/Robust-Prefix-Tuning)] 

    *Zonghan Yang, Yang Liu.* 

1. **[[lfll'22]](https://openreview.net/pdf?id=HCRVf71PMF)** LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. ICLR 2022. [[code](https://github.com/qcwthu/Lifelong-Fewshot-Language-Learning)] 

    *Chengwei Qin, Shafiq Joty.* 



### ICML 2022

1. **[[PGC'22]](http://proceedings.mlr.press/v139/wang21g.html)** Self-Tuning for Data-Efficient Deep Learning. ICML 2022. [[code](https://github.com/thuml/Self-Tuning)]  [[Slides](https://icml.cc/media/icml-2021/Slides/8615.pdf)] 

    *Ximei Wang · Jinghan Gao · Mingsheng Long · Jianmin Wang.* 

1. **[[Model soups'22]](https://proceedings.mlr.press/v162/wortsman22a.html)** Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. ICML 2022. [[code](https://github.com/mlfoundations/model-soups)]

    *Mitchell Wortsman Gabriel Ilharco Samir Yitzhak Gadre Rebecca Roelofs Raphael Gontijo-Lopes Ari S. Morcos 4 Hongseok Namkoong Ali Farhadi Yair Carmon Simon Kornblith Ludwig Schmidt* 

1. **[[BBT'22]](https://proceedings.mlr.press/v162/sun22e.html)** Black-Box Tuning for Language-Model-as-a-Service. ICML 2022. [[code](https://github.com/txsun1997/Black-Box-Tuning)] [[Poster](https://icml.cc/media/PosterPDFs/ICML%202022/ef575e8837d065a1683c022d2077d342_GbkiyvM.png)] 

    *Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu*

1. **[[XX'22]](https://proceedings.mlr.press/v162/ju22a.html)** Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees. ICML 2022. 

    *Haotian Ju, Dongyue Li, Hongyang R Zhang.* 

1. **[[TLM'22]](https://proceedings.mlr.press/v162/yao22c.html)** NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework. ICML 2022. [[code](https://github.com/yaoxingcheng/TLM)] [[Poster](https://icml.cc/media/PosterPDFs/ICML%202022/05311655a15b75fab86956663e1819cd_0njXElx.png)]

    *Xingcheng Yao, Yanan Zheng, Xiaocong Yang, Zhilin Yang.* 



### NeurIPS 2022 (TBD)


### ICLR 2023 (TBD)

